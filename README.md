Introduction scraping de donnÃ©es
================================

Ce repo contient les notebooks correspondant au brief [La Plume Libre #3] Extraction de donnÃ©es automatisÃ©e pour le site d'une librairie en ligne.


# ğŸ“š Book Scraper

Ce projet est un pipeline de scraping structurÃ© pour extraire, nettoyer et stocker les donnÃ©es des livres du site [books.toscrape.com](http://books.toscrape.com) dans une base de donnÃ©es SQLite.

---

## ğŸ§  Objectifs du projet

- Scraper les informations des livres (titre, prix, note, disponibilitÃ©).
- Nettoyer les donnÃ©es pour un usage ultÃ©rieur (analyse, visualisation, etc.).
- InsÃ©rer les donnÃ©es dans une base de donnÃ©es locale (`book_store.db`).
- Stocker les donnÃ©es brutes au format CSV (`data/books_infos.csv`).

---

## ğŸ“ Arborescence du projet

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ books_infos.csv            # DonnÃ©es brutes au format CSV
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ insert_data.py             # Insertion des donnÃ©es dans la base SQLite
â”‚   â””â”€â”€ book_store.db              # Base de donnÃ©es SQLite gÃ©nÃ©rÃ©e
â”œâ”€â”€ get_data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ get_scraping_data.py       # Fonction de rÃ©cupÃ©ration HTML
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_scraping.ipynb           # Notebook pour tests de scraping
â”‚   â”œâ”€â”€ 2_create_bdd.ipynb         # Notebook pour crÃ©ation de la base
â”‚   â””â”€â”€ 3_API_googleBooks.ipynb    # Test d'enrichissement via l'API Google Books
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pipeline_scraping.py       # Pipeline complet d'extraction + insertion
â”œâ”€â”€ process_data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ process_scraping_data.py   # Nettoyage et typage des donnÃ©es
â”œâ”€â”€ main.py                        # Script principal Ã  exÃ©cuter
â”œâ”€â”€ README.md                      # Documentation du projet
â”œâ”€â”€ requirements.txt

---
```

## âš™ï¸ Installation

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/loicgoi/scraping
   
   cd book-scraper

   python -m venv venv

   pip install -r requirements.txt


## â–¶ï¸ Lancer le projet

    Dans le fichier main.py, le pipeline est lancÃ© avec le nombre de pages Ã  scraper et une URL modifiable :

    from pipelines.pipeline_scraping import run_scraping_pipeline

    if __name__ == "__main__":
        base_url = "http://books.toscrape.com/catalogue/page-{}.html"
        df = run_scraping_pipeline(pages=50, base_url=base_url)
    
    Puis exÃ©cuter simplement :

    python main.py

## ğŸ’¡ Personnalisation
    Pour changer de site, modifiez base_url dans main.py. Assurez-vous que la structure HTML soit compatible avec la fonction parse_books_html().

    Pour scraper plus ou moins de pages, changez la valeur de pages.

## ğŸ§ª Exemple de donnÃ©es extraites
    title	price	rating	availability
    It's Only the Himalayas	45.17	2	True
    Tipping the Velvet	53.74	1	True
    Soumission	50.10	1	True

## ğŸ—ƒï¸ Base de donnÃ©es
    Les donnÃ©es nettoyÃ©es sont insÃ©rÃ©es dans une base SQLite locale : db/book_store.db.

    La table principale est nommÃ©e book_store.

## ğŸ› ï¸ Technologies utilisÃ©es
    Python 3.10+

    requests, beautifulsoup4, pandas, sqlite3

## ğŸ“„ Licence
    Projet Ã  but pÃ©dagogique - libre d'utilisation et de modification.

## ğŸ™‹â€â™€ï¸ Auteurs
    Projet dÃ©veloppÃ© par [loicgoi]
    Formation IA - Simplon Montpellier (2025â€“2026)